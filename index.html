<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Eastany.GitHub.io</h1>
      <h2 class="project-tagline">golang - c - python -ng</h2>
    </section>

    <section class="main-content">
      <h3>
<a id="just-for-golang" class="anchor" href="#just-for-golang" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Golang.</h3>

<p>golang使用的那些坑,调度实现,内存,cgo?,tcp/ip,分布式?连续栈?</p>

<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>C</h3>

<p>cgroups是control groups的缩写，是Linux内核提供的一种可以限制、记录、隔离进程组（process groups）所使用的物理资源（如：cpu,memory,IO等等）的机制。上篇博客写的namespace作用是使linux的全局资源局域化，使各个名字空间的系统环境相互隔离，互不影响。而cgroup可以限制资源使用的最大值，限制当前进程组对外的最大影响（但无法隔离其他进程对自己的影响）。namespace与cgroup相互配合将使容器具备环境隔离和资源限制的能力，再加上镜像提供的根目录环境，使用chroot即可以为容器提供一个隔离的根目录环境。 本文将主要从内核源码的角度分析docker常用的cgroup资源限制功能：cpuset memory 和blkio。
cgroup 子系统介绍
blkio – 这个子系统为块设备设定输入/输出限制，比如物理设备（磁盘，固态硬盘，USB 等等）。 cpu – 这个子系统控制cgroup中所有进程可以使用的时间片。 cpuacct – 这个子系统自动生成 cgroup 中任务所使用的 CPU 报告。 cpuset – 这个子系统为 cgroup 中的任务分配独立 CPU（在多核系统）和内存节点。 devices – 这个子系统可允许或者拒绝 cgroup 中的任务访问设备。 freezer – 这个子系统挂起或者恢复 cgroup 中的任务。 memory – 这个子系统设定 cgroup 中任务使用的内存限制，并自动生成由那些任务使用的内存资源报告。 net_cls – 这个子系统使用等级识别符（classid）标记网络数据包，可允许 Linux 流量控制程序（tc）识别从具体 cgroup 中生成的数据包。 ns – 名称空间子系统。 cgroup系统是一个树状结构，子cgroup的限定条件必须为父cgroup的子集</p>
<p>cpuset子系统
我们先看一下cpuset数据结构：
1
2
3
4
5
6
7
8
struct cpuset {
        struct cgroup_subsys_state css;
 ...//忽略一些代码
        cpumask_var_t cpus_allowed;
        nodemask_t mems_allowed;
...//忽略一些代码
      
};
忽略了一些代码，重点关注cpuset的cpus_allowed。这个结构用于限定一个进程组所能使用cpu核。通过task_struct，进程都能找到进程所在进程组的cpu限制和内存节点限制，具体的内核数据结构比较复杂，这里略过。 linux内核是通过将进程加入对应cpu核的任务队列实现进程与cpu核绑定功能的。新建的进程与已存在的进程调度略有不同。我们知道，内核是通过do_fork实现新建进程，在do_fork函数里调用 wake_up_new_task实现新建进程的调度。wake_up_new_task相关代码如下：
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
/*
 * wake_up_new_task - wake up a newly created task for the first time.
 *
  * This function will do some initial scheduler statistics housekeeping
  * that must be done for every newly created context, then puts the task
  * on the runqueue and wakes it.
  */
 void wake_up_new_task(struct task_struct *p)
 {
        unsigned long flags;
        struct rq *rq;
        raw_spin_lock_irqsave(&p->pi_lock, flags);
 #ifdef CONFIG_SMP
        /*
         * Fork balancing, do it here and not earlier because:
         *  - cpus_allowed can change in the fork path
         *  - any previously selected cpu might disappear through hotplug
        */
         set_task_cpu(p, select_task_rq(p, task_cpu(p), SD_BALANCE_FORK, 0));
 #endif
 
         /* Initialize new task's runnable average */
         init_task_runnable_average(p);
         rq = __task_rq_lock(p);
         activate_task(rq, p, 0);
         p->on_rq = TASK_ON_RQ_QUEUED;
         trace_sched_wakeup_new(p, true);
         check_preempt_curr(rq, p, WF_FORK);
 #ifdef CONFIG_SMP
        if (p->sched_class->task_woken)
            p->sched_class->task_woken(rq, p);
 #endif
         task_rq_unlock(rq, p, &flags);
 }
从代码中可以看出，内核是通过调用
1
set_task_cpu(p, select_task_rq(p, task_cpu(p), SD_BALANCE_FORK, 0));
实现进程在特定cpu核上运行的。select_task_rq返回的是进程被允许运行的一个合适的cpu id。set_task_cpu会使该进程运行在该cpu核上。
对于已经存在的进程，实现的原理也大致相同。对于完全公平调度CFS算法，内核是通过调用select_task_rq_fair从允许的cpu核中选择一个合适的cpu id返回，然后加入任务队列，等待调度运行。
memory子系统
memory子系统是通过linux的resource counter机制实现的。在具体实现的过程中，cgroup通过内核中的resource counter机制实现内存的限制。resource counter相当于一个通用的资源计数器，在内核中通过res_counter结构来描述。该结构可用于记录某类资源的当前使用量、最大使用量以及上限等信息。mem_cgroup定义如下：
1
2
3
4
5
6
7
8
9
10
struct mem_cgroup {
...
// the counter to account for memory usage
 struct res_counter res;
 //the counter to account for mem+swap usage.
struct res_counter memsw;
//the counter to account for kernel memory usage.
struct res_counter kmem;
...
}
res_counter定义如下：
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
struct res_counter {
          /*
           * the current resource consumption level
           */
          unsigned long long usage;
          /*
           * the maximal value of the usage from the counter creation
           */
          unsigned long long max_usage;
          /*
           * the limit that usage cannot exceed
           */
          unsigned long long limit;
          /*
           * the limit that usage can be exceed
           */
          unsigned long long soft_limit;
          /*
           * the number of unsuccessful attempts to consume the resource
           */
          unsigned long long failcnt;
          /*
           * the lock to protect all of the above.
           * the routines below consider this to be IRQ-safe
           */
          spinlock_t lock;
          /*
           * Parent counter, used for hierarchial resource accounting
           */
          struct res_counter *parent;
  };
res_counter中的每个字段表示对内存使用量的记录。用户态下memory子系统所导出的配置文件与该结构中的字段互相对应，比如mem.limit_in_bytes表示当前cgroup可使用内存的最大上线，该文件与res_counter结构中的limit字段对应。也就是说，当用户在用户态向mem.limit_in_bytes文件写入值后，则res_counter中的limit字段相应更新。
内核对res_counter进行操作时有三个基本函数：res_counter_init()对res_counter进行初始化；当分配资源时，res_counter_charge()记录资源的使用量，并且该函数还会检查使用量是否超过了上限，并且记录当前资源使用量的最大值；当资源被释放时，res_counter_uncharge()则减少该资源的使用量。
res_counter_charge调用res_counter_charge_locked函数判断当前进程组的资源使用量是否超出限制。
1
2
3
4
5
6
7
8
9
10
11
12
int res_counter_charge_locked(struct res_counter *counter, unsigned long val)
  {
          if (counter->usage + val > counter->limit) {
                  counter->failcnt++;
                  return -ENOMEM;
          }
  
          counter->usage += val;
          if (counter->usage > counter->max_usage)
                  counter->max_usage = counter->usage;
          return 0;
  }
从代码中可以看出，当进程组的资源使用量超出limit值时内核将会返回ENOMEN错误，并根据目前的资源使用量决定是否更新max_usage值。 这里需要注意的是memory系统是限制实际分配的物理内存大小,而非应用程序申请的虚拟内存。只有真正申请物理内存时（page demand 或者copy on write等)，该内存才会被计入res_counter。应用程序调用malloc申请的是虚拟内存，实际使用了多少物理内存必须从内核获得：/proc目录。因此，在docker里可以malloc到远大于限制值的虚拟内存，但当实际要使用超过设置值的内存时该进程会被内核的OOM（out of memory ）killer机制杀死，关于OOM机制可以查阅相关的文档，在做相关测试时需要注意虚拟内存大小与实际物理内存大小的区别。
blkio子系统
blkio子系统可以设置块设备的两个维度： 1.Weight值。可以对不同的设备设置不同的权重，让权重更高的设备执行更多的IO操作 2.IO操作速度上限值。可以设置块设备的最大读写速度。 这里主要分析IO操作的速度限制原理。 cgroup限制IO速度大小的具体原理是设置IO队列，每隔一段时间（throtl_slice ，目前为0.1s）检查是否超过设定值，如果超过就将IO操作加入等待队列，等待一段时间再进行IO操作。 主要控制函数为tg_may_dispatch：
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
 /*
* Returns whether one can dispatch a bio or not. Also returns approx number
  * of jiffies to wait before this bio is with-in IO rate and can be dispatched
 */
 static bool tg_may_dispatch(struct throtl_grp *tg, struct bio *bio,
                             unsigned long *wait)
 {
        bool rw = bio_data_dir(bio);
        unsigned long bps_wait = 0, iops_wait = 0, max_wait = 0;
        /*
          * Currently whole state machine of group depends on first bio
          * queued in the group bio list. So one should not be calling
          * this function with a different bio if there are other bios
          * queued.
          */
         BUG_ON(tg->service_queue.nr_queued[rw] &&
                bio != throtl_peek_queued(&tg->service_queue.queued[rw]));
         /* If tg->bps = -1, then BW is unlimited */
         if (tg->bps[rw] == -1 && tg->iops[rw] == -1) {
                 if (wait)
                         *wait = 0;
                 return true;
         }
         /*
          * If previous slice expired, start a new one otherwise renew/extend
          * existing slice to make sure it is at least throtl_slice interval
          * long since now.
          */
         if (throtl_slice_used(tg, rw))
                 throtl_start_new_slice(tg, rw);
         else {
                 if (time_before(tg->slice_end[rw], jiffies + throtl_slice))
                         throtl_extend_slice(tg, rw, jiffies + throtl_slice);
         }
         if (tg_with_in_bps_limit(tg, bio, &bps_wait) &&
            tg_with_in_iops_limit(tg, bio, &iops_wait)) {
                 if (wait)
                         *wait = 0;
                return 1;
         }
         max_wait = max(bps_wait, iops_wait);
         if (wait)
                *wait = max_wait;
 
         if (time_before(tg->slice_end[rw], jiffies + max_wait))
                throtl_extend_slice(tg, rw, jiffies + max_wait);
 
         return 0;
 }
代码注释写的比较清晰。该函数主要判断是否应该下发IO操作，并且获得需要等待的时间。因此blkio子系统限制IO速度是通过限定其平均值实现的。可以推测，当在throtl_slice(0.1s)时间内进行远大于设定值的IO操作时，cgroup是无法限制住的，有一定的局限性。</p>

<h3>
<a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Ng与Html5</h3>

<p>If you prefer to not use the automatic generator, push a branch named <code>gh-pages</code> to your repository to create a page manually. In addition to supporting regular HTML content, GitHub Pages support Jekyll, a simple, blog aware static site generator. Jekyll makes it easy to create site-wide headers and footers without having to copy them across every page. It also offers intelligent blog support and other advanced templating features.</p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Authors and Contributors</h3>

<p>You can <a href="https://help.github.com/articles/basic-writing-and-formatting-syntax/#mentioning-users-and-teams" class="user-mention">@mention</a> a GitHub username to generate a link to their profile. The resulting <code>&lt;a&gt;</code> element will link to the contributor’s GitHub Profile. For example: In 2007, Chris Wanstrath (<a href="https://github.com/defunkt" class="user-mention">@defunkt</a>), PJ Hyett (<a href="https://github.com/pjhyett" class="user-mention">@pjhyett</a>), and Tom Preston-Werner (<a href="https://github.com/mojombo" class="user-mention">@mojombo</a>) founded GitHub.</p>

<h3>
<a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Support or Contact</h3>

<p>Having trouble with Pages? Check out our <a href="https://help.github.com/pages">documentation</a> or <a href="https://github.com/contact">contact support</a> and we’ll help you sort it out.</p>

      <footer class="site-footer">

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
